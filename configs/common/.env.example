# Environment variables for mistral.rs deployment
# Copy this file to .env and adjust values for your environment

# ============================================================================
# Server Configuration
# ============================================================================

# HTTP server settings
MISTRALRS_PORT=8080
MISTRALRS_HOST=0.0.0.0
MISTRALRS_WORKERS=2

# Maximum concurrent sequences
MISTRALRS_MAX_SEQS=256

# ============================================================================
# Model Configuration
# ============================================================================

# Model directory (contains .gguf or safetensors files)
MODEL_DIR=/models
# Or for Windows: C:\codedev\llm\.models\qwen2.5-1.5b-it-gguf

# Model filename (for GGUF models)
MODEL_FILE=Qwen2.5-1.5B-Instruct-Q4_K_M.gguf

# Model ID (for Hugging Face models)
# MODEL_ID=meta-llama/Llama-3.2-3B-Instruct

# ============================================================================
# GPU Configuration
# ============================================================================

# CUDA device selection (0 for first GPU, -1 for CPU only)
CUDA_VISIBLE_DEVICES=0

# Enable CUDA memory management
CUDA_LAUNCH_BLOCKING=0

# ============================================================================
# Rust Runtime Configuration
# ============================================================================

# Logging level (error, warn, info, debug, trace)
RUST_LOG=info

# Backtrace on panic (0 or 1 or full)
RUST_BACKTRACE=1

# ============================================================================
# MCP Server Configuration
# ============================================================================

# Path to MCP configuration file
MCP_CONFIG=/config/mcp-config.json

# MCP protocol version
MCP_PROTOCOL_VERSION=2025-06-18

# ============================================================================
# Cache Configuration
# ============================================================================

# Hugging Face hub cache directory
HUGGINGFACE_HUB_CACHE=/data/hf_cache
# Or for Windows: C:\Users\david\.cache\huggingface

# Model cache directory
MODEL_CACHE_DIR=/data/model_cache

# ============================================================================
# Security & Authentication
# ============================================================================

# API keys (if authentication is enabled)
# API_KEY=your-api-key-here

# GitHub token (for GitHub MCP server)
# GITHUB_PERSONAL_ACCESS_TOKEN=ghp_your_token_here

# Redis connection (for RAG)
# REDIS_URL=redis://127.0.0.1:6379

# ============================================================================
# Monitoring & Observability
# ============================================================================

# Prometheus metrics endpoint
# METRICS_PORT=9090

# Enable performance tracking
# ENABLE_METRICS=true

# ============================================================================
# Resource Limits
# ============================================================================

# Maximum memory usage (in MB)
# MAX_MEMORY_MB=8192

# Thread pool size
# THREAD_POOL_SIZE=16

# ============================================================================
# Development Settings
# ============================================================================

# Enable debug mode
# DEBUG=false

# Hot reload models
# HOT_RELOAD=false

# Verbose logging
# VERBOSE=false
